# 工作过程
1. 缩小尺寸：pHash以小图片开始，但图片大于8\*8，32\*32是最好的。这样做的目的是简化了DCT的计算，而不是减小频率。  
2. 简化色彩：将图片转化成灰度图像，进一步简化计算量。  
3. 计算DCT：计算图片的DCT变换，得到32\*32的DCT系数矩阵。  
4. 缩小DCT：虽然DCT的结果是32\*32大小的矩阵，但我们只要保留左上角的8\*8的矩阵，这部分呈现了图片中的最低频率。  
5. 计算平均值：如同均值哈希一样，计算DCT的均值。  
6. 计算hash值：这是最主要的一步，根据8\*8的DCT矩阵，设置0或1的64位的hash值，大于等于DCT均值的设为”1”，小于DCT均值的设为“0”。组合在一起，就构成了一个64位的整数，这就是这张图片的指纹。  
7. 计算汉明距离：计算两张图片hash值的汉明距离，汉明距离小于5可认为相似。

结果并不能告诉我们真实性的低频率，只能粗略地告诉我们相对于平均值频率的相对比例。只要图片的整体结构保持不变，hash结果值就不变。能够避免伽马校正或颜色直方图被调整带来的影响。pHash可以用汉明距离来进行比较。(只需要比较每一位对应的位置并算计不同的位的个数)  
参考：[perceptual hash 感知哈希算法](https://www.cnblogs.com/faith0217/articles/4088386.html)

# python实现
```
# -*- coding: utf-8 -*-
import cv2
import matplotlib
from PIL import Image
from matplotlib import pyplot
import numpy as np
import os
import shutil
import json

FILEEXTENTION = ['png', 'jpg']

# 计算图片hash值
def getImgHash(fileName):
    size = (32, 32)
    im = cv2.imread(fileName, 0)
    im = cv2.resize(im, size)
    imData = im.astype(np.float32)
    dct = cv2.dct(imData)

    dctList = []
    sum = 0
    for i in range(8):
        for j in range(8):
            dctList.append(dct[i][j])
            sum += dct[i][j]

    avg = sum / 64
    hashStr = ''
    for item in dctList:
        hashStr += '0' if item < avg else '1'
    return int(hashStr, 2)

# 计算汉明距离
def calHammingDistance(h1, h2):
    n = h1 ^ h2
    return bin(n).count('1')

def getImgHashDic(filePath):
    imgHashDic = {}
    for root, dirs, files in os.walk(filePath):
        for file in files:
            if file.split('.')[-1] in FILEEXTENTION and file[-7:-4] != '_X2':
                imgPath = os.path.join(root, file)
                imgHash = getImgHash(imgPath)
                imgHashDic[imgPath] = imgHash
    return imgHashDic

def getDuplicationImg(imgHashDic):
    dupImgList = []
    knownImgList = []
    count = 0
    for item1 in imgHashDic:
        if item1 in knownImgList:
            continue
        imgList = []
        for item2 in imgHashDic:
            if item1 == item2 or item2 in knownImgList:
                continue
            if calHammingDistance(imgHashDic[item1], imgHashDic[item2]) < 2:
                imgList.append(item2)
        if len(imgList) > 0:
            imgList.append(item1)
            dupImgList.append(imgList)
            knownImgList.extend(imgList)
            dupImgPath = os.path.join('Dup', str(count))
            if os.path.exists(dupImgPath):
                shutil.rmtree(dupImgPath)
            os.makedirs(dupImgPath)
            for img in imgList:
                shutil.copy(img, os.path.join(dupImgPath, img.split(os.sep)[-1]))
            count += 1
    f = open('dumImg.json', 'w')
    json.dump(dupImgList, f, indent=4)


if __name__ == '__main__':
    imgHashDic = getImgHashDic('./Resource')
    print('get hash end')
    getDuplicationImg(imgHashDic)

```